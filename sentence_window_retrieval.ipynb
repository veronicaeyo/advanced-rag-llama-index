{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from scripts import utils\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"pdfs/eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "41 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: d62c0dde-aae2-48e9-9f17-b20839a711a2\n",
      "Text: PAGE 1Founder, DeepLearning.AICollected Insights from Andrew Ng\n",
      "How to  Build Your Career in AIA Simple Guide\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "# join it into 1 single list\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window-sentence retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello. how are you? I am fine!  \"\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello. ', 'how are you? ', 'I am fine!  ']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.  how are you?  I am fine!  \n"
     ]
    }
   ],
   "source": [
    "print(nodes[1].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "#     window_size=2,\n",
    "#     window_metadata_key=\"window\",\n",
    "#     original_text_metadata_key=\"original_text\",\n",
    "# )\n",
    "\n",
    "# text = \"hello. foo bar. cat dog. mouse\"\n",
    "\n",
    "# nodes = node_parser.get_nodes_from_documents([Document(text=text)])\n",
    "# print([x.text for x in nodes])\n",
    "# print(nodes[0].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "# vector store created( it need the llm, embedding model and node parser)\n",
    "sentence_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    # embed_model=\"local:BAAI/bge-large-en-v1.5\"\n",
    "    node_parser=node_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "sentence_index = VectorStoreIndex.from_documents(\n",
    "    [document], service_context=sentence_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is optional to check\n",
    "# if an index file exist, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "import os\n",
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "if not os.path.exists(\"./sentence_index\"):\n",
    "    sentence_index = VectorStoreIndex.from_documents(\n",
    "        [document], service_context=sentence_context\n",
    "    )\n",
    "\n",
    "    sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n",
    "else:\n",
    "    sentence_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./sentence_index\"),\n",
    "        service_context=sentence_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the post processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "postproc = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello. ', 'how are you? ', 'I am fine!  ']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "# replicating similarity search result\n",
    "scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]\n",
    "nodes_old = [deepcopy(n) for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how are you? '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_old[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_nodes = postproc.postprocess_nodes(scored_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.  how are you?  I am fine!  \n"
     ]
    }
   ],
   "source": [
    "print(replaced_nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# BAAI/bge-reranker-base\n",
    "# link: https://huggingface.co/BAAI/bge-reranker-base\n",
    "rerank = SentenceTransformerRerank(\n",
    "    top_n=3, model=\"BAAI/bge-reranker-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a reranker uses question and document as input  for similarity analysis\n",
    "\n",
    "### The cell below is used to test a query with its scores and returns a correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "query = QueryBundle(\"I want a dog.\")\n",
    "\n",
    "scored_nodes = [\n",
    "    NodeWithScore(node=TextNode(text=\"This is a cat\"), score=0.6),\n",
    "    NodeWithScore(node=TextNode(text=\"This is a dog\"), score=0.4),\n",
    "    NodeWithScore(node=TextNode(text=\"A dog is domestic animal\"), score=0.6),\n",
    "    NodeWithScore(node=TextNode(text=\"This is a lion\"), score=0.4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_nodes = rerank.postprocess_nodes(\n",
    "    scored_nodes, query_bundle=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This is a dog', 0.91827345), ('A dog is domestic animal', 0.6579715), ('This is a lion', 0.001732392)]\n"
     ]
    }
   ],
   "source": [
    "print([(x.text, x.score) for x in reranked_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"What are the keys to building a career in AI?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The keys to building a career in AI are discussed in Chapter 10 of the book."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    save_dir=\"./sentence_index\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The main skills needed to build a career in AI include learning foundational '\n",
      " 'technical skills, working on projects, and finding a job. Additionally, '\n",
      " 'being part of a supportive community can also be beneficial. It is important '\n",
      " 'to stay curious and continue learning, even if something may not seem '\n",
      " 'immediately useful. The ability to identify ideas worth working on is also a '\n",
      " 'valuable skill for an AI architect.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "pprint(query_engine.query(\"what are the main skills needed to build a carrer in AI\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruLens Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('text/generated_questions.text', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///sentence_window.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "tru = Tru(database_url=\"sqlite:///db/sentence_window.sqlite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.base_query_engine import BaseQueryEngine\n",
    "from trulens_eval import TruLlama\n",
    "from typing import List\n",
    "\n",
    "def run_evals(eval_questions: List[str], tru_recorder, query_engine: BaseQueryEngine):\n",
    "    for question in eval_questions:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import get_prebuilt_trulens_recorder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence window size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_1 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=1,\n",
    "    save_dir=\"sentence_index_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine_1 = get_sentence_window_query_engine(\n",
    "    sentence_index_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder_1 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_1,\n",
    "    app_id='sentence window engine 1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder_1, sentence_window_engine_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d92a06a1454e47952eb2359eb9863b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dashboard failed to start in time. Please inspect dashboard logs for additional information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Tru()\u001b[38;5;241m.\u001b[39mrun_dashboard()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\tru.py:689\u001b[0m, in \u001b[0;36mTru.run_dashboard\u001b[1;34m(self, port, address, force, _dev)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m started\u001b[38;5;241m.\u001b[39mwait(timeout\u001b[38;5;241m=\u001b[39mwait_period):\n\u001b[0;32m    688\u001b[0m     Tru\u001b[38;5;241m.\u001b[39mdashboard_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDashboard failed to start in time. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease inspect dashboard logs for additional information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dashboard failed to start in time. Please inspect dashboard logs for additional information."
     ]
    }
   ],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence window size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_3 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index_3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine_3 = get_sentence_window_query_engine(\n",
    "    sentence_index_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder_3 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_3,\n",
    "    app_id='sentence window engine 3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error calling wrapped function get_response.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 146, in get_response\n",
      "    response = self._give_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 202, in _give_response_single\n",
      "    program(\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 64, in __call__\n",
      "    answer = self._llm.predict(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py\", line 239, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 100, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 237, in chat\n",
      "    return chat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 296, in _chat\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 846, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 270, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 877, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 901, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 929, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 966, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 1002, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 228, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 262, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 245, in handle_request\n",
      "    response = connection.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 133, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 111, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 176, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 212, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 126, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py\", line 1296, in recv\n",
      "    return self.read(buflen)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py\", line 1169, in read\n",
      "    return self._sslobj.read(len)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "Error calling wrapped function get_response.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\compact_and_refine.py\", line 38, in get_response\n",
      "    return super().get_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 793, in tru_wrapper\n",
      "    raise error\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 146, in get_response\n",
      "    response = self._give_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 202, in _give_response_single\n",
      "    program(\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 64, in __call__\n",
      "    answer = self._llm.predict(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py\", line 239, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 100, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 237, in chat\n",
      "    return chat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 296, in _chat\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 846, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 270, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 877, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 901, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 929, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 966, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 1002, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 228, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 262, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 245, in handle_request\n",
      "    response = connection.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 133, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 111, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 176, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 212, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 126, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py\", line 1296, in recv\n",
      "    return self.read(buflen)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py\", line 1169, in read\n",
      "    return self._sslobj.read(len)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "Error calling wrapped function query.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\core\\base_query_engine.py\", line 40, in query\n",
      "    return self._query(str_or_query_bundle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py\", line 172, in _query\n",
      "    response = self._response_synthesizer.synthesize(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\base.py\", line 162, in synthesize\n",
      "    response_str = self.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 793, in tru_wrapper\n",
      "    raise error\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\compact_and_refine.py\", line 38, in get_response\n",
      "    return super().get_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 793, in tru_wrapper\n",
      "    raise error\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 146, in get_response\n",
      "    response = self._give_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 202, in _give_response_single\n",
      "    program(\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 64, in __call__\n",
      "    answer = self._llm.predict(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py\", line 239, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 100, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 237, in chat\n",
      "    return chat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 296, in _chat\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 846, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 270, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 877, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 901, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 929, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 966, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py\", line 1002, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 228, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 262, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 245, in handle_request\n",
      "    response = connection.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 133, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 111, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 176, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 212, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 126, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py\", line 1296, in recv\n",
      "    return self.read(buflen)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py\", line 1169, in read\n",
      "    return self._sslobj.read(len)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "Unsure what the main output string is for the call to query.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3)\n",
      "Cell \u001b[1;32mIn[55], line 7\u001b[0m, in \u001b[0;36mrun_evals\u001b[1;34m(eval_questions, tru_recorder, query_engine)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_evals\u001b[39m(eval_questions: List[\u001b[38;5;28mstr\u001b[39m], tru_recorder, query_engine: BaseQueryEngine):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m eval_questions:\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tru_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[0;32m      8\u001b[0m             response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(question)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\app.py:735\u001b[0m, in \u001b[0;36mApp.__exit__\u001b[1;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording_contexts\u001b[38;5;241m.\u001b[39mreset(ctx\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m, in \u001b[0;36mrun_evals\u001b[1;34m(eval_questions, tru_recorder, query_engine)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m eval_questions:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tru_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[1;32m----> 8\u001b[0m         response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(question)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:781\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;66;03m# If stack has only 1 thing on it, we are looking at a \"root\u001b[39;00m\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;66;03m# call\". Create a record of the result and notify the app:\u001b[39;00m\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;66;03m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;66;03m# into its containers:\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39m_on_add_record(\n\u001b[0;32m    782\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    783\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    784\u001b[0m             sig\u001b[38;5;241m=\u001b[39msig,\n\u001b[0;32m    785\u001b[0m             bindings\u001b[38;5;241m=\u001b[39mbindings,\n\u001b[0;32m    786\u001b[0m             ret\u001b[38;5;241m=\u001b[39mrets,\n\u001b[0;32m    787\u001b[0m             error\u001b[38;5;241m=\u001b[39merror,\n\u001b[0;32m    788\u001b[0m             perf\u001b[38;5;241m=\u001b[39mPerf(start_time\u001b[38;5;241m=\u001b[39mstart_time, end_time\u001b[38;5;241m=\u001b[39mend_time),\n\u001b[0;32m    789\u001b[0m             cost\u001b[38;5;241m=\u001b[39mcost\n\u001b[0;32m    790\u001b[0m         )\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\app.py:783\u001b[0m, in \u001b[0;36mApp._on_add_record\u001b[1;34m(self, ctx, func, sig, bindings, ret, error, perf, cost)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# May block on DB.\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(record\u001b[38;5;241m=\u001b[39mrecord, error\u001b[38;5;241m=\u001b[39merror)\n\u001b[1;32m--> 783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    785\u001b[0m \u001b[38;5;66;03m# Will block on DB, but not on feedback evaluation, depending on\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# FeedbackMode:\u001b[39;00m\n\u001b[0;32m    787\u001b[0m record\u001b[38;5;241m.\u001b[39mfeedback_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_record(record\u001b[38;5;241m=\u001b[39mrecord)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:732\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:462\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[0;32m    450\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# TODO: dedup async/sync\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs(\n\u001b[0;32m    463\u001b[0m         thunk,\n\u001b[0;32m    464\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[0;32m    465\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[0;32m    466\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[0;32m    467\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:411\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_track_costs(thunk, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:568\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[1;34m(thunk, with_endpoints)\u001b[0m\n\u001b[0;32m    565\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m result: T \u001b[38;5;241m=\u001b[39m thunk()\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:733\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[1;32m--> 733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\core\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:172\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    169\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m    171\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 172\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    173\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    174\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    177\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\base.py:162\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    160\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    161\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 162\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    163\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    164\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    165\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    166\u001b[0m         ],\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    171\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:793\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39m_on_add_record(\n\u001b[0;32m    782\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    783\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m             cost\u001b[38;5;241m=\u001b[39mcost\n\u001b[0;32m    790\u001b[0m         )\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:732\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:462\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[0;32m    450\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# TODO: dedup async/sync\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs(\n\u001b[0;32m    463\u001b[0m         thunk,\n\u001b[0;32m    464\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[0;32m    465\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[0;32m    466\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[0;32m    467\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:411\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_track_costs(thunk, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:568\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[1;34m(thunk, with_endpoints)\u001b[0m\n\u001b[0;32m    565\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m result: T \u001b[38;5;241m=\u001b[39m thunk()\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:733\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[1;32m--> 733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     39\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     40\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     41\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:793\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39m_on_add_record(\n\u001b[0;32m    782\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    783\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m             cost\u001b[38;5;241m=\u001b[39mcost\n\u001b[0;32m    790\u001b[0m         )\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:732\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:462\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[0;32m    450\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# TODO: dedup async/sync\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs(\n\u001b[0;32m    463\u001b[0m         thunk,\n\u001b[0;32m    464\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[0;32m    465\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[0;32m    466\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[0;32m    467\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:411\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_track_costs(thunk, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:568\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[1;34m(thunk, with_endpoints)\u001b[0m\n\u001b[0;32m    565\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m result: T \u001b[38;5;241m=\u001b[39m thunk()\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:733\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[1;32m--> 733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py:146\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[0;32m    147\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    148\u001b[0m         )\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[0;32m    151\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[0;32m    152\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    153\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py:202\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[1;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    201\u001b[0m             StructuredRefineResponse,\n\u001b[1;32m--> 202\u001b[0m             program(\n\u001b[0;32m    203\u001b[0m                 context_str\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[0;32m    204\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    205\u001b[0m             ),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py:64\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     62\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py:239\u001b[0m, in \u001b[0;36mLLM.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[0;32m    238\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 239\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat(messages)\n\u001b[0;32m    240\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py:100\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[0;32m     92\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m     93\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m     94\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m         },\n\u001b[0;32m     99\u001b[0m     )\n\u001b[1;32m--> 100\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py:237\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py:296\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m    295\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[1;32m--> 296\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    297\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts,\n\u001b[0;32m    298\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    300\u001b[0m )\n\u001b[0;32m    301\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    302\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:846\u001b[0m, in \u001b[0;36mEndpoint.wrap_function.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling wrapped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    845\u001b[0m \u001b[38;5;66;03m# Get the result of the wrapped function:\u001b[39;00m\n\u001b[1;32m--> 846\u001b[0m response: Any \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    848\u001b[0m bindings \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    850\u001b[0m \u001b[38;5;66;03m# Get all of the callback classes suitable for handling this call.\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Note that we stored this in the INSTRUMENT attribute of the\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# wrapper method.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py:270\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    643\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    647\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    648\u001b[0m             {\n\u001b[0;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    669\u001b[0m             },\n\u001b[0;32m    670\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    671\u001b[0m         ),\n\u001b[0;32m    672\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    673\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    674\u001b[0m         ),\n\u001b[0;32m    675\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    676\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    677\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    678\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    854\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    855\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    856\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    857\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    858\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    859\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(request)\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    878\u001b[0m         request,\n\u001b[0;32m    879\u001b[0m         auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth,\n\u001b[0;32m    880\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    881\u001b[0m     )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    897\u001b[0m )\n\u001b[0;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    902\u001b[0m     request,\n\u001b[0;32m    903\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    904\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    905\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    906\u001b[0m )\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    930\u001b[0m         request,\n\u001b[0;32m    931\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    932\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    933\u001b[0m     )\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    964\u001b[0m     hook(request)\n\u001b[1;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m     )\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpx\\_transports\\default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    226\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    237\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:262\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:245\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    106\u001b[0m     (\n\u001b[0;32m    107\u001b[0m         http_version,\n\u001b[0;32m    108\u001b[0m         status,\n\u001b[0;32m    109\u001b[0m         reason_phrase,\n\u001b[0;32m    110\u001b[0m         headers,\n\u001b[1;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    113\u001b[0m         http_version,\n\u001b[0;32m    114\u001b[0m         status,\n\u001b[0;32m    115\u001b[0m         reason_phrase,\n\u001b[0;32m    116\u001b[0m         headers,\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     },\n\u001b[0;32m    128\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.1.74:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence window engine 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence window engine 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.70</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Answer Relevance  Groundedness  Context Relevance  \\\n",
       "app_id                                                                        \n",
       "sentence window engine 3               1.0      0.925000               0.85   \n",
       "sentence window engine 1               1.0      0.422222               0.70   \n",
       "\n",
       "                          latency  total_cost  \n",
       "app_id                                         \n",
       "sentence window engine 3     22.0    0.001383  \n",
       "sentence window engine 1     22.0    0.001053  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- put all steps in window size put it in a single  function where sentence widow size is the only parameter taht changes and use string formatting to change the app id and test for sentence window sixze of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_run_evals(window_size):\n",
    "    sentence_index = build_sentence_window_index(\n",
    "        documents,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "        embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "        sentence_window_size=window_size,\n",
    "        save_dir=f\"sentence_index_{window_size}\"\n",
    "    )\n",
    "    \n",
    "    sentence_window_engine = get_sentence_window_query_engine(sentence_index)\n",
    "    \n",
    "    tru_recorder = get_prebuilt_trulens_recorder(\n",
    "        sentence_window_engine,\n",
    "        app_id=f'sentence window engine {window_size}'\n",
    "    )\n",
    "    \n",
    "    run_evals(eval_questions, tru_recorder, sentence_window_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_run_evals(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence window engine 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence window engine 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence window engine 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.70</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Answer Relevance  Groundedness  Context Relevance  \\\n",
       "app_id                                                                        \n",
       "sentence window engine 5               1.0      1.000000               0.90   \n",
       "sentence window engine 3               1.0      0.925000               0.85   \n",
       "sentence window engine 1               1.0      0.422222               0.70   \n",
       "\n",
       "                          latency  total_cost  \n",
       "app_id                                         \n",
       "sentence window engine 5     22.0    0.001715  \n",
       "sentence window engine 3     22.0    0.001383  \n",
       "sentence window engine 1     22.0    0.001053  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x1dff4e13090 is calling an instrumented method <function BaseQueryEngine.query at 0x000001DFAFCEAC00>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x1dffc2ea4d0) using this function.\n",
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x1dff4e13090 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x000001DFB5FE0220>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x1dffc2ea4d0) using this function.\n",
      "A new object of type <class 'llama_index.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> at 0x1dff4e13f10 is calling an instrumented method <function BaseRetriever.retrieve at 0x000001DFB2743100>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.retriever based on other object (0x1dfd9108b10) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x1dff4e13c50 is calling an instrumented method <function CompactAndRefine.get_response at 0x000001DFB27239C0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x1dfd9108a50) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x1dff4e13c50 is calling an instrumented method <function Refine.get_response at 0x000001DFB2740EA0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x1dfd9108a50) using this function.\n"
     ]
    }
   ],
   "source": [
    "build_run_evals(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence window engine 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence window engine 7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.90</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence window engine 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence window engine 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.70</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Answer Relevance  Groundedness  Context Relevance  \\\n",
       "app_id                                                                        \n",
       "sentence window engine 5               1.0      1.000000               0.90   \n",
       "sentence window engine 7               1.0      0.928571               0.90   \n",
       "sentence window engine 3               1.0      0.925000               0.85   \n",
       "sentence window engine 1               1.0      0.422222               0.70   \n",
       "\n",
       "                          latency  total_cost  \n",
       "app_id                                         \n",
       "sentence window engine 5     22.0    0.001715  \n",
       "sentence window engine 7     22.0    0.002123  \n",
       "sentence window engine 3     22.0    0.001383  \n",
       "sentence window engine 1     22.0    0.001053  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.1.74:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dashboard not running in this workspace. You may be able to shut other instances by setting the `force` flag.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tru\u001b[38;5;241m.\u001b[39mstop_dashboard()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\tru.py:429\u001b[0m, in \u001b[0;36mTru.stop_dashboard\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Tru\u001b[38;5;241m.\u001b[39mdashboard_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force:\n\u001b[1;32m--> 429\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDashboard not running in this workspace. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may be able to shut other instances by setting the `force` flag.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         )\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Dashboard not running in this workspace. You may be able to shut other instances by setting the `force` flag."
     ]
    }
   ],
   "source": [
    "tru.stop_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
