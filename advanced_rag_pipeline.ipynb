{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f5727c057e4ca0b42bababbd8674d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ve797\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75aa352b451342a2ab6c845e35429c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b94c6dd8824f839a25b1d24ebdbbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52a52ca927141e2ada0e2a74073a379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a5b3f8acb14b1dbea3eefc58753863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c0a02466a7498898f066cd7f82fcf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "# rerank = SentenceTransformer Rerank(top_n=2, model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from scripts import utils\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"pdfs/eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "41 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: ec100ef6-32f6-4eb0-8fba-cfbbf29c33aa\n",
      "Text: PAGE 1Founder, DeepLearning.AICollected Insights from Andrew Ng\n",
      "How to  Build Your Career in AIA Simple Guide\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(document.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f6b19d1694b8c88fe1c784b47788b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ve797\\AppData\\Local\\llama_index\\models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c6223a14064dbcb208fad8cebdbaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0596ccb8df4d7ba55b6a248548deb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674807d96fd44bf684834310d9d2b418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7d231ff7474f809f0a831d3b3f2c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dbf18823564789b6c39467406452f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "index = VectorStoreIndex.from_documents([document],\n",
    "                                        service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('When finding projects to build your experience, there are several steps you '\n",
      " 'can take. First, you can join existing projects by asking to join someone '\n",
      " \"else's project if they have an idea. Additionally, you can continue reading, \"\n",
      " 'taking courses, and talking to domain experts to come up with new ideas. It '\n",
      " 'is also helpful to focus on an application area that has not yet been '\n",
      " 'explored with machine learning. If your company or school has a particular '\n",
      " 'application in mind, you can explore the possibilities for machine learning '\n",
      " 'in that area. Finally, you can develop a side hustle or personal project '\n",
      " 'that may not initially be part of your job but can help you gain experience '\n",
      " 'and strengthen your skills.')\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are steps to take when finding projects to build your experience?\"\n",
    ")\n",
    "pprint(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval setup using TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the keys to building a career in AI?\n",
      "How can teamwork contribute to success in AI?\n",
      "What is the importance of networking in AI?\n",
      "What are some good habits to develop for a successful career?\n",
      "How can altruism be beneficial in building a career?\n",
      "What is imposter syndrome and how does it relate to AI?\n",
      "Who are some accomplished individuals who have experienced imposter syndrome?\n",
      "What is the first step to becoming good at AI?\n",
      "What are some common challenges in AI?\n",
      "Is it normal to find parts of AI challenging?\n"
     ]
    }
   ],
   "source": [
    "eval_questions = []\n",
    "with open('text/eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character \n",
    "        item = line.strip()\n",
    "        print(item)\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try your own question:\n",
    "new_question = \"What is the right AI job for me?\"\n",
    "eval_questions.append(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are the keys to building a career in AI?',\n",
      " 'How can teamwork contribute to success in AI?',\n",
      " 'What is the importance of networking in AI?',\n",
      " 'What are some good habits to develop for a successful career?',\n",
      " 'How can altruism be beneficial in building a career?',\n",
      " 'What is imposter syndrome and how does it relate to AI?',\n",
      " 'Who are some accomplished individuals who have experienced imposter '\n",
      " 'syndrome?',\n",
      " 'What is the first step to becoming good at AI?',\n",
      " 'What are some common challenges in AI?',\n",
      " 'Is it normal to find parts of AI challenging?',\n",
      " 'What is the right AI job for me?']\n"
     ]
    }
   ],
   "source": [
    "pprint(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import get_prebuilt_trulens_recorder\n",
    "\n",
    "tru_recorder = get_prebuilt_trulens_recorder(query_engine,\n",
    "                                             app_id=\"Direct Query Engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "Error calling wrapped function get_response.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 146, in get_response\n",
      "    response = self._give_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 202, in _give_response_single\n",
      "    program(\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 64, in __call__\n",
      "    answer = self._llm.predict(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py\", line 239, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 100, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 237, in chat\n",
      "    return chat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 296, in _chat\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 846, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 270, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "Error calling wrapped function get_response.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\compact_and_refine.py\", line 38, in get_response\n",
      "    return super().get_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 793, in tru_wrapper\n",
      "    raise error\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 146, in get_response\n",
      "    response = self._give_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 202, in _give_response_single\n",
      "    program(\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 64, in __call__\n",
      "    answer = self._llm.predict(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py\", line 239, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 100, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 237, in chat\n",
      "    return chat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 296, in _chat\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 846, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 270, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "Error calling wrapped function query.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\core\\base_query_engine.py\", line 40, in query\n",
      "    return self._query(str_or_query_bundle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py\", line 172, in _query\n",
      "    response = self._response_synthesizer.synthesize(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\base.py\", line 162, in synthesize\n",
      "    response_str = self.get_response(\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 793, in tru_wrapper\n",
      "    raise error\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\compact_and_refine.py\", line 38, in get_response\n",
      "    return super().get_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 793, in tru_wrapper\n",
      "    raise error\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 732, in tru_wrapper\n",
      "    rets, cost = Endpoint.track_all_costs_tally(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 462, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 411, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 568, in _track_costs\n",
      "    result: T = thunk()\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py\", line 733, in <lambda>\n",
      "    lambda: func(*bindings.args, **bindings.kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 146, in get_response\n",
      "    response = self._give_response_single(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 202, in _give_response_single\n",
      "    program(\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py\", line 64, in __call__\n",
      "    answer = self._llm.predict(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py\", line 239, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 100, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 237, in chat\n",
      "    return chat_fn(messages, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py\", line 296, in _chat\n",
      "    response = client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py\", line 846, in wrapper\n",
      "    response: Any = func(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 270, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 916, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "Unsure what the main output string is for the call to query.\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tru_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m eval_questions:\n\u001b[0;32m      3\u001b[0m         response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(question)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\app.py:735\u001b[0m, in \u001b[0;36mApp.__exit__\u001b[1;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording_contexts\u001b[38;5;241m.\u001b[39mreset(ctx\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tru_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m eval_questions:\n\u001b[1;32m----> 3\u001b[0m         response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(question)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:781\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;66;03m# If stack has only 1 thing on it, we are looking at a \"root\u001b[39;00m\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;66;03m# call\". Create a record of the result and notify the app:\u001b[39;00m\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;66;03m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;66;03m# into its containers:\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39m_on_add_record(\n\u001b[0;32m    782\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    783\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    784\u001b[0m             sig\u001b[38;5;241m=\u001b[39msig,\n\u001b[0;32m    785\u001b[0m             bindings\u001b[38;5;241m=\u001b[39mbindings,\n\u001b[0;32m    786\u001b[0m             ret\u001b[38;5;241m=\u001b[39mrets,\n\u001b[0;32m    787\u001b[0m             error\u001b[38;5;241m=\u001b[39merror,\n\u001b[0;32m    788\u001b[0m             perf\u001b[38;5;241m=\u001b[39mPerf(start_time\u001b[38;5;241m=\u001b[39mstart_time, end_time\u001b[38;5;241m=\u001b[39mend_time),\n\u001b[0;32m    789\u001b[0m             cost\u001b[38;5;241m=\u001b[39mcost\n\u001b[0;32m    790\u001b[0m         )\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\app.py:783\u001b[0m, in \u001b[0;36mApp._on_add_record\u001b[1;34m(self, ctx, func, sig, bindings, ret, error, perf, cost)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# May block on DB.\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(record\u001b[38;5;241m=\u001b[39mrecord, error\u001b[38;5;241m=\u001b[39merror)\n\u001b[1;32m--> 783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    785\u001b[0m \u001b[38;5;66;03m# Will block on DB, but not on feedback evaluation, depending on\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# FeedbackMode:\u001b[39;00m\n\u001b[0;32m    787\u001b[0m record\u001b[38;5;241m.\u001b[39mfeedback_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_record(record\u001b[38;5;241m=\u001b[39mrecord)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:732\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:462\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[0;32m    450\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# TODO: dedup async/sync\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs(\n\u001b[0;32m    463\u001b[0m         thunk,\n\u001b[0;32m    464\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[0;32m    465\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[0;32m    466\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[0;32m    467\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:411\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_track_costs(thunk, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:568\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[1;34m(thunk, with_endpoints)\u001b[0m\n\u001b[0;32m    565\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m result: T \u001b[38;5;241m=\u001b[39m thunk()\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:733\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[1;32m--> 733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\core\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:172\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    169\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m    171\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 172\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    173\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    174\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    177\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\base.py:162\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    160\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    161\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 162\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    163\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    164\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    165\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    166\u001b[0m         ],\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    171\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:793\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39m_on_add_record(\n\u001b[0;32m    782\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    783\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m             cost\u001b[38;5;241m=\u001b[39mcost\n\u001b[0;32m    790\u001b[0m         )\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:732\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:462\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[0;32m    450\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# TODO: dedup async/sync\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs(\n\u001b[0;32m    463\u001b[0m         thunk,\n\u001b[0;32m    464\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[0;32m    465\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[0;32m    466\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[0;32m    467\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:411\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_track_costs(thunk, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:568\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[1;34m(thunk, with_endpoints)\u001b[0m\n\u001b[0;32m    565\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m result: T \u001b[38;5;241m=\u001b[39m thunk()\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:733\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[1;32m--> 733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     39\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     40\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     41\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:793\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39m_on_add_record(\n\u001b[0;32m    782\u001b[0m             ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    783\u001b[0m             func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m             cost\u001b[38;5;241m=\u001b[39mcost\n\u001b[0;32m    790\u001b[0m         )\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:732\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:462\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs_tally\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_all_costs_tally\u001b[39m(\n\u001b[0;32m    450\u001b[0m     thunk: Thunk[T],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[T, Cost]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# TODO: dedup async/sync\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    execution of thunk.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     result, cbs \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs(\n\u001b[0;32m    463\u001b[0m         thunk,\n\u001b[0;32m    464\u001b[0m         with_openai\u001b[38;5;241m=\u001b[39mwith_openai,\n\u001b[0;32m    465\u001b[0m         with_hugs\u001b[38;5;241m=\u001b[39mwith_hugs,\n\u001b[0;32m    466\u001b[0m         with_litellm\u001b[38;5;241m=\u001b[39mwith_litellm,\n\u001b[0;32m    467\u001b[0m         with_bedrock\u001b[38;5;241m=\u001b[39mwith_bedrock\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         costs \u001b[38;5;241m=\u001b[39m Cost()\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:411\u001b[0m, in \u001b[0;36mEndpoint.track_all_costs\u001b[1;34m(thunk, with_openai, with_hugs, with_litellm, with_bedrock)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not initiallize endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossibly missing key(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrulens_eval will not track costs/usage of this endpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Endpoint\u001b[38;5;241m.\u001b[39m_track_costs(thunk, with_endpoints\u001b[38;5;241m=\u001b[39mendpoints)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:568\u001b[0m, in \u001b[0;36mEndpoint._track_costs\u001b[1;34m(thunk, with_endpoints)\u001b[0m\n\u001b[0;32m    565\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(callback)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Call the thunk.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m result: T \u001b[38;5;241m=\u001b[39m thunk()\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Return result and only the callbacks created here. Outer thunks might\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# return others.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, callbacks\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\instruments.py:733\u001b[0m, in \u001b[0;36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Using sig bind here so we can produce a list of key-value\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# pairs even if positional arguments were provided.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     bindings: BoundArguments \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    732\u001b[0m     rets, cost \u001b[38;5;241m=\u001b[39m Endpoint\u001b[38;5;241m.\u001b[39mtrack_all_costs_tally(\n\u001b[1;32m--> 733\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbindings\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    734\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    737\u001b[0m     error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py:146\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[0;32m    147\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    148\u001b[0m         )\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[0;32m    151\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[0;32m    152\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    153\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py:202\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[1;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    201\u001b[0m             StructuredRefineResponse,\n\u001b[1;32m--> 202\u001b[0m             program(\n\u001b[0;32m    203\u001b[0m                 context_str\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[0;32m    204\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    205\u001b[0m             ),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\response_synthesizers\\refine.py:64\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     62\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\llm.py:239\u001b[0m, in \u001b[0;36mLLM.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[0;32m    238\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 239\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat(messages)\n\u001b[0;32m    240\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\base.py:100\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[0;32m     92\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m     93\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m     94\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m         },\n\u001b[0;32m     99\u001b[0m     )\n\u001b[1;32m--> 100\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py:237\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\llama_index\\llms\\openai.py:296\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m    295\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[1;32m--> 296\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    297\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts,\n\u001b[0;32m    298\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    300\u001b[0m )\n\u001b[0;32m    301\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    302\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\trulens_eval\\feedback\\provider\\endpoint\\base.py:846\u001b[0m, in \u001b[0;36mEndpoint.wrap_function.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling wrapped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    845\u001b[0m \u001b[38;5;66;03m# Get the result of the wrapped function:\u001b[39;00m\n\u001b[1;32m--> 846\u001b[0m response: Any \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    848\u001b[0m bindings \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    850\u001b[0m \u001b[38;5;66;03m# Get all of the callback classes suitable for handling this call.\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Note that we stored this in the INSTRUMENT attribute of the\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# wrapper method.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_utils\\_utils.py:270\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    643\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    647\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    648\u001b[0m             {\n\u001b[0;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    669\u001b[0m             },\n\u001b[0;32m    670\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    671\u001b[0m         ),\n\u001b[0;32m    672\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    673\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    674\u001b[0m         ),\n\u001b[0;32m    675\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    676\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    677\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    678\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    854\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    855\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    856\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    857\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    858\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    859\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    917\u001b[0m         options,\n\u001b[0;32m    918\u001b[0m         cast_to,\n\u001b[0;32m    919\u001b[0m         retries,\n\u001b[0;32m    920\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    921\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    922\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    923\u001b[0m     )\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    960\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    961\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m    962\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    963\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    964\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    917\u001b[0m         options,\n\u001b[0;32m    918\u001b[0m         cast_to,\n\u001b[0;32m    919\u001b[0m         retries,\n\u001b[0;32m    920\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    921\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    922\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    923\u001b[0m     )\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    960\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    961\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m    962\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    963\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    964\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    917\u001b[0m         options,\n\u001b[0;32m    918\u001b[0m         cast_to,\n\u001b[0;32m    919\u001b[0m         retries,\n\u001b[0;32m    920\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    921\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    922\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    923\u001b[0m     )\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    960\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    961\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m    962\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    963\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    964\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ve797\\anaconda4\\envs\\openai\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-cbE5rpB12OCY5Q0iIRAY6hNo on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    for question in eval_questions[:2]:\n",
    "        response = query_engine.query(question)\n",
    "\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
